{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac0690f-5058-4244-a080-34de890ddaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Common Hyperparameters of Decision Tree Models and Their Effects\n",
    "\n",
    "Decision Tree models (like sklearn.tree.DecisionTreeClassifier) have several key hyperparameters:\n",
    "\n",
    "\n",
    "* max_depth - Limits how deep the tree can grow.\n",
    "\n",
    "Effect: Small depth â†’ simpler tree (less overfitting, but may underfit).\n",
    "        Large depth â†’ complex tree (more accurate on training, risk of overfitting).\n",
    "\n",
    "* min_samples_split - Minimum number of samples required to split a node.\n",
    "\n",
    "Effect: Higher value â†’ fewer splits (simpler model, less variance, more bias).\n",
    "        Lower value â†’ more splits (complex model, less bias, more variance).\n",
    "\n",
    "* min_samples_leaf - Minimum number of samples required in a leaf node.\n",
    "\n",
    "Effect: Larger value â†’ more smoothing, prevents tiny leaf nodes.\n",
    "        Smaller value â†’ tree can capture fine patterns (risk of overfitting).\n",
    "\n",
    "* max_features - Number of features considered at each split.\n",
    "\n",
    "Effect: Smaller value â†’ increases randomness, can reduce overfitting.\n",
    "        Larger value â†’ model may overfit.\n",
    "\n",
    "* criterion (e.g., \"gini\" or \"entropy\") - Metric used to measure the quality of a split.\n",
    "\n",
    "Effect: Both usually give similar results, but \"entropy\" is more computationally expensive.\n",
    "\n",
    "* max_leaf_nodes - Maximum number of leaf nodes in the tree.\n",
    "\n",
    "Effect: Limits complexity â†’ prevents overfitting.\n",
    "\n",
    "* ccp_alpha (Cost Complexity Pruning) - Controls pruning of the tree.\n",
    "\n",
    "Effect: Higher value â†’ more pruning â†’ simpler model.\n",
    "\n",
    "ðŸ“Œ Summary:\n",
    "\n",
    "Small values (shallow tree, more restrictions) â†’ less overfitting but higher bias.\n",
    "\n",
    "Large values (deep tree, fewer restrictions) â†’ more overfitting but lower bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c93cb5-407a-4c20-8464-8cf11c6d7c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Difference Between Label Encoding and One-Hot Encoding\n",
    "\n",
    "* Label Encoding - Converts categorical values into numeric labels (integers).\n",
    "\n",
    "* Example: Color = [Red, Blue, Green]  \n",
    "         Label Encoded â†’ [2, 0, 1]\n",
    "\n",
    "\n",
    "* Advantages: Simple, memory-efficient.\n",
    "\n",
    "* Disadvantages: Implies an ordinal relationship (e.g., Blue < Green < Red), which may mislead the model.\n",
    "                 Works better for ordinal categorical features (like \"Low\", \"Medium\", \"High\").\n",
    "\n",
    "* One-Hot Encoding - Converts each category into a separate binary column (dummy variable).\n",
    "\n",
    "* Example: Color = [Red, Blue, Green]  \n",
    "            One-Hot Encoded â†’  \n",
    "            Red   Blue   Green  \n",
    "             1     0      0  \n",
    "             0     1      0  \n",
    "             0     0      1\n",
    "\n",
    "\n",
    "* Advantages: No ordinal relationship introduced.\n",
    "              Works well with nominal (unordered) categories.\n",
    "\n",
    "* Disadvantages: Increases dimensionality (curse of dimensionality for high-cardinality features)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
